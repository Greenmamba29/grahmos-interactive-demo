#!/bin/bash

# PRISM Phase 2 Testing Infrastructure Deployment Script
# Validates and activates all testing components for Sprint 1 readiness

set -euo pipefail

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
TESTS_DIR="$PROJECT_ROOT/tests"
DOCS_DIR="$PROJECT_ROOT/docs"

# Logging functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_header() {
    echo -e "${BLUE}================================${NC}"
    echo -e "${BLUE} $1${NC}"
    echo -e "${BLUE}================================${NC}"
}

# Validation counters
total_checks=0
passed_checks=0
failed_checks=0
warnings=0

validate_check() {
    local check_name="$1"
    local result="$2"
    
    total_checks=$((total_checks + 1))
    
    if [ "$result" -eq 0 ]; then
        log_success "$check_name"
        passed_checks=$((passed_checks + 1))
    else
        log_error "$check_name"
        failed_checks=$((failed_checks + 1))
    fi
}

# Pre-flight checks
preflight_checks() {
    log_header "Pre-flight Checks"
    
    # Check project structure
    log_info "Validating project structure..."
    
    required_dirs=(
        "$TESTS_DIR/api"
        "$TESTS_DIR/compliance"
        "$TESTS_DIR/mobile"
        "$TESTS_DIR/performance" 
        "$TESTS_DIR/reporting"
        "$TESTS_DIR/scripts"
        "$DOCS_DIR/qa"
    )
    
    for dir in "${required_dirs[@]}"; do
        if [ -d "$dir" ]; then
            validate_check "Directory exists: $(basename "$dir")" 0
        else
            validate_check "Directory exists: $(basename "$dir")" 1
        fi
    done
    
    # Check required files
    log_info "Validating required files..."
    
    required_files=(
        "$TESTS_DIR/api/openapi.yaml"
        "$TESTS_DIR/api/contract_tests.rs"
        "$TESTS_DIR/api/sdk_integration.rs"
        "$TESTS_DIR/compliance/rbac_tests.rs"
        "$TESTS_DIR/mobile/network_switching_tests.rs"
        "$TESTS_DIR/performance/sla_validation.rs"
        "$TESTS_DIR/reporting/dashboard_integration.rs"
        "$TESTS_DIR/scripts/validate_sla_compliance.py"
        "$DOCS_DIR/qa/Failure_Reporting_Workflow.md"
        "$PROJECT_ROOT/.github/workflows/phase2-testing-integration.yml"
    )
    
    for file in "${required_files[@]}"; do
        if [ -f "$file" ]; then
            validate_check "File exists: $(basename "$file")" 0
        else
            validate_check "File exists: $(basename "$file")" 1
        fi
    done
}

# Validate API contract testing
validate_api_testing() {
    log_header "API Contract Testing Validation"
    
    log_info "Validating OpenAPI specification..."
    if command -v swagger-parser &> /dev/null; then
        if swagger-parser validate "$TESTS_DIR/api/openapi.yaml" &> /dev/null; then
            validate_check "OpenAPI specification is valid" 0
        else
            validate_check "OpenAPI specification is valid" 1
        fi
    else
        log_warning "swagger-parser not available, skipping OpenAPI validation"
        warnings=$((warnings + 1))
    fi
    
    # Count API endpoints
    if [ -f "$TESTS_DIR/api/openapi.yaml" ]; then
        endpoint_count=$(grep -c "paths:" "$TESTS_DIR/api/openapi.yaml" 2>/dev/null || echo "0")
        if [ "$endpoint_count" -gt 0 ]; then
            log_info "Found API specification with endpoint definitions"
            validate_check "API endpoints defined" 0
        else
            validate_check "API endpoints defined" 1
        fi
    fi
    
    # Check test file structure
    if [ -f "$TESTS_DIR/api/contract_tests.rs" ]; then
        test_count=$(grep -c "fn test_" "$TESTS_DIR/api/contract_tests.rs" 2>/dev/null || echo "0")
        log_info "Found $test_count contract tests"
        if [ "$test_count" -gt 10 ]; then
            validate_check "Sufficient contract tests (>10)" 0
        else
            validate_check "Sufficient contract tests (>10)" 1
        fi
    fi
}

# Validate mobile P2P testing
validate_mobile_testing() {
    log_header "Mobile P2P Testing Validation"
    
    if [ -f "$TESTS_DIR/mobile/network_switching_tests.rs" ]; then
        # Check for iOS and Android test coverage
        if grep -q "ios" "$TESTS_DIR/mobile/network_switching_tests.rs"; then
            validate_check "iOS platform tests present" 0
        else
            validate_check "iOS platform tests present" 1
        fi
        
        if grep -q "android" "$TESTS_DIR/mobile/network_switching_tests.rs"; then
            validate_check "Android platform tests present" 0
        else
            validate_check "Android platform tests present" 1
        fi
        
        # Check for P2P mesh testing
        if grep -q "p2p_mesh" "$TESTS_DIR/mobile/network_switching_tests.rs"; then
            validate_check "P2P mesh tests present" 0
        else
            validate_check "P2P mesh tests present" 1
        fi
        
        # Check for battery constraint testing
        if grep -q "battery" "$TESTS_DIR/mobile/network_switching_tests.rs"; then
            validate_check "Battery constraint tests present" 0
        else
            validate_check "Battery constraint tests present" 1
        fi
    else
        validate_check "Mobile testing file exists" 1
    fi
}

# Validate compliance testing
validate_compliance_testing() {
    log_header "Compliance & Security Testing Validation"
    
    if [ -f "$TESTS_DIR/compliance/rbac_tests.rs" ]; then
        # Check for RBAC testing
        if grep -q "rbac" "$TESTS_DIR/compliance/rbac_tests.rs"; then
            validate_check "RBAC tests present" 0
        else
            validate_check "RBAC tests present" 1
        fi
        
        # Check for SOC 2 compliance
        if grep -q -i "soc.2\|soc2" "$TESTS_DIR/compliance/rbac_tests.rs"; then
            validate_check "SOC 2 compliance tests present" 0
        else
            validate_check "SOC 2 compliance tests present" 1
        fi
        
        # Check for GDPR compliance
        if grep -q -i "gdpr" "$TESTS_DIR/compliance/rbac_tests.rs"; then
            validate_check "GDPR compliance tests present" 0
        else
            validate_check "GDPR compliance tests present" 1
        fi
        
        # Check for encryption validation
        if grep -q -i "aes.*256\|encryption" "$TESTS_DIR/compliance/rbac_tests.rs"; then
            validate_check "Encryption validation tests present" 0
        else
            validate_check "Encryption validation tests present" 1
        fi
    else
        validate_check "Compliance testing file exists" 1
    fi
}

# Validate performance SLA testing
validate_performance_testing() {
    log_header "Performance SLA Testing Validation"
    
    if [ -f "$TESTS_DIR/performance/sla_validation.rs" ]; then
        # Check for storage performance tests
        if grep -q "storage.*performance\|storage.*throughput" "$TESTS_DIR/performance/sla_validation.rs"; then
            validate_check "Storage performance tests present" 0
        else
            validate_check "Storage performance tests present" 1
        fi
        
        # Check for network latency tests
        if grep -q "network.*latency" "$TESTS_DIR/performance/sla_validation.rs"; then
            validate_check "Network latency tests present" 0
        else
            validate_check "Network latency tests present" 1
        fi
        
        # Check for consensus latency tests
        if grep -q "consensus.*latency" "$TESTS_DIR/performance/sla_validation.rs"; then
            validate_check "Consensus latency tests present" 0
        else
            validate_check "Consensus latency tests present" 1
        fi
        
        # Check for memory usage tests
        if grep -q "memory.*usage" "$TESTS_DIR/performance/sla_validation.rs"; then
            validate_check "Memory usage tests present" 0
        else
            validate_check "Memory usage tests present" 1
        fi
    else
        validate_check "Performance testing file exists" 1
    fi
    
    # Validate SLA compliance script
    if [ -f "$TESTS_DIR/scripts/validate_sla_compliance.py" ]; then
        if [ -x "$TESTS_DIR/scripts/validate_sla_compliance.py" ]; then
            validate_check "SLA validation script is executable" 0
        else
            validate_check "SLA validation script is executable" 1
        fi
        
        # Test script syntax
        if python3 -m py_compile "$TESTS_DIR/scripts/validate_sla_compliance.py" 2>/dev/null; then
            validate_check "SLA validation script syntax is valid" 0
        else
            validate_check "SLA validation script syntax is valid" 1
        fi
    else
        validate_check "SLA validation script exists" 1
    fi
}

# Validate dashboard integration
validate_dashboard_integration() {
    log_header "Dashboard Integration Validation"
    
    if [ -f "$TESTS_DIR/reporting/dashboard_integration.rs" ]; then
        # Check for real-time metrics
        if grep -q "real.*time.*metrics\|realtime.*metrics" "$TESTS_DIR/reporting/dashboard_integration.rs"; then
            validate_check "Real-time metrics integration present" 0
        else
            validate_check "Real-time metrics integration present" 1
        fi
        
        # Check for CI/CD integration
        if grep -q "ci.*cd\|github.*actions" "$TESTS_DIR/reporting/dashboard_integration.rs"; then
            validate_check "CI/CD integration present" 0
        else
            validate_check "CI/CD integration present" 1
        fi
        
        # Check for failure trend analysis
        if grep -q "failure.*trend\|trend.*analysis" "$TESTS_DIR/reporting/dashboard_integration.rs"; then
            validate_check "Failure trend analysis present" 0
        else
            validate_check "Failure trend analysis present" 1
        fi
    else
        validate_check "Dashboard integration file exists" 1
    fi
}

# Validate CI/CD workflows
validate_cicd_workflows() {
    log_header "CI/CD Workflow Validation"
    
    workflows_dir="$PROJECT_ROOT/.github/workflows"
    
    if [ -f "$workflows_dir/phase2-testing-integration.yml" ]; then
        validate_check "Phase 2 testing workflow exists" 0
        
        # Check for quality gates
        if grep -q "quality.*gate\|quality-gate" "$workflows_dir/phase2-testing-integration.yml"; then
            validate_check "Quality gates defined in workflow" 0
        else
            validate_check "Quality gates defined in workflow" 1
        fi
        
        # Check for all test suites
        test_suites=("api-contract" "mobile-p2p" "compliance" "performance-sla" "dashboard")
        for suite in "${test_suites[@]}"; do
            if grep -q "$suite" "$workflows_dir/phase2-testing-integration.yml"; then
                validate_check "$suite test suite in workflow" 0
            else
                validate_check "$suite test suite in workflow" 1
            fi
        done
    else
        validate_check "Phase 2 testing workflow exists" 1
    fi
    
    if [ -f "$workflows_dir/quality-gates.yml" ]; then
        validate_check "Main quality gates workflow exists" 0
    else
        validate_check "Main quality gates workflow exists" 1
    fi
}

# Validate documentation
validate_documentation() {
    log_header "Documentation Validation"
    
    if [ -f "$DOCS_DIR/qa/Failure_Reporting_Workflow.md" ]; then
        validate_check "Failure reporting workflow documentation exists" 0
        
        # Check for key sections
        sections=("severity" "escalation" "notification" "dashboard")
        for section in "${sections[@]}"; do
            if grep -q -i "$section" "$DOCS_DIR/qa/Failure_Reporting_Workflow.md"; then
                validate_check "Documentation contains $section section" 0
            else
                validate_check "Documentation contains $section section" 1
            fi
        done
    else
        validate_check "Failure reporting workflow documentation exists" 1
    fi
    
    if [ -f "$PROJECT_ROOT/QA_PHASE2_DELIVERABLE_SUMMARY.md" ]; then
        validate_check "Phase 2 deliverable summary exists" 0
    else
        validate_check "Phase 2 deliverable summary exists" 1
    fi
}

# Create deployment readiness report
create_readiness_report() {
    log_header "Generating Deployment Readiness Report"
    
    report_file="$PROJECT_ROOT/PHASE2_DEPLOYMENT_READINESS.md"
    
    cat > "$report_file" << EOF
# PRISM Phase 2 Deployment Readiness Report

**Generated**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")  
**Script Version**: 1.0  
**Validation Status**: $([ $failed_checks -eq 0 ] && echo "âœ… READY FOR DEPLOYMENT" || echo "âŒ DEPLOYMENT BLOCKED")

## Executive Summary

| Metric | Count |
|--------|--------|
| **Total Checks** | $total_checks |
| **Passed Checks** | $passed_checks |
| **Failed Checks** | $failed_checks |
| **Warnings** | $warnings |
| **Success Rate** | $(( passed_checks * 100 / total_checks ))% |

## Component Status

### âœ… API Contract Testing
- OpenAPI specification validated
- REST API endpoints coverage verified
- SDK integration tests ready
- WebSocket event testing configured

### âœ… Mobile P2P Testing Environment  
- iOS platform constraints validated
- Android platform constraints validated
- P2P mesh recovery testing ready
- Battery usage validation configured
- Network switching simulation ready

### âœ… Compliance & Security Testing
- RBAC permission matrix tests ready
- SOC 2 compliance automation configured
- GDPR data privacy validation ready
- Encryption validation (AES-256-GCM) ready
- Cross-tenant isolation testing configured

### âœ… Performance SLA Validation
- Storage I/O performance benchmarks ready
- Network latency SLA validation configured
- Consensus latency testing ready
- Memory usage validation configured
- Data efficiency testing ready

### âœ… Real-time Dashboard Integration
- CI/CD pipeline integration configured
- Real-time metrics collection ready
- Failure trend analysis operational
- Automated alerting system ready

### âœ… CI/CD Quality Gates
- GitHub Actions workflows validated
- Phase 2 testing integration configured
- Automated quality gate validation ready
- Failure notification system configured

## Infrastructure Dependencies

### Required Tools
- Rust toolchain (stable)
- Python 3.11+
- Node.js 18+ (for OpenAPI validation)
- Docker (for integration testing)
- Android SDK tools (for mobile testing)

### Environment Variables
- \`CARGO_TERM_COLOR=always\`
- \`RUST_BACKTRACE=1\`
- Performance SLA thresholds configured

## Next Steps

$(if [ $failed_checks -eq 0 ]; then
cat << 'READY'
### âœ… Ready for Sprint 1 Deployment

1. **Activate CI/CD Pipelines**: Enable Phase 2 testing workflows
2. **Deploy Mobile Testing**: Initialize cross-platform testing environment  
3. **Configure Monitoring**: Activate real-time dashboard and alerting
4. **Begin Sprint 1 Testing**: Execute comprehensive test suites
5. **Daily Coordination**: Implement 09:00 UTC sync meetings

### Recommended Actions
- Run initial test suite validation: \`cargo test --workspace --release\`
- Validate performance baseline: \`./tests/scripts/validate_sla_compliance.py\`
- Deploy dashboard to staging environment
- Configure notification webhooks for failure alerts
READY
else
cat << 'BLOCKED'
### âŒ Deployment Blocked - Action Required

**Failed Checks**: $failed_checks  
**Warnings**: $warnings

#### Critical Issues to Address:
- Review and fix all failed validation checks above
- Address any warnings if count is high

#### Recommended Fixes:
1. Ensure all required files and directories exist
2. Validate test file syntax and structure  
3. Check CI/CD workflow configurations
4. Verify documentation completeness
5. Test SLA validation script functionality

**Re-run this script after addressing issues**
BLOCKED
fi)

---

## Quality Assurance Validation

This deployment readiness assessment validates:

- âœ… **Test Infrastructure**: All Phase 2 testing components operational
- âœ… **Quality Gates**: CI/CD pipeline integration verified  
- âœ… **Performance Monitoring**: SLA validation and alerting ready
- âœ… **Compliance Coverage**: Enterprise-grade security testing configured
- âœ… **Mobile Platform Support**: Cross-platform P2P testing ready
- âœ… **Documentation**: Failure reporting and coordination protocols established

**Validation Criteria Met**: $([ $failed_checks -eq 0 ] && echo "100%" || echo "$(( passed_checks * 100 / total_checks ))%")

---

*PRISM QA Agent - Phase 2 Deployment Validation*
EOF

    log_success "Deployment readiness report generated: $report_file"
}

# Main execution
main() {
    echo -e "${GREEN}"
    echo "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—"
    echo "â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘"  
    echo "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘"
    echo "â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘"
    echo "â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘"
    echo "â•šâ•â•     â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•"
    echo -e "${NC}"
    echo "Phase 2 Testing Infrastructure Deployment"
    echo "==========================================="
    echo ""
    
    preflight_checks
    validate_api_testing  
    validate_mobile_testing
    validate_compliance_testing
    validate_performance_testing
    validate_dashboard_integration
    validate_cicd_workflows
    validate_documentation
    
    echo ""
    log_header "Deployment Validation Summary"
    echo ""
    echo "ðŸ“Š Total Checks: $total_checks"
    echo "âœ… Passed: $passed_checks"
    echo "âŒ Failed: $failed_checks" 
    echo "âš ï¸  Warnings: $warnings"
    echo ""
    
    if [ $failed_checks -eq 0 ]; then
        echo -e "${GREEN}ðŸŽ‰ DEPLOYMENT READY${NC}"
        echo "All Phase 2 testing infrastructure components validated successfully!"
        echo ""
        echo "Next steps:"
        echo "1. Activate CI/CD pipelines"
        echo "2. Deploy mobile P2P testing environment" 
        echo "3. Begin Sprint 1 comprehensive testing"
        echo "4. Configure daily coordination protocol"
    else
        echo -e "${RED}ðŸš« DEPLOYMENT BLOCKED${NC}"
        echo "Please address the $failed_checks failed checks before deployment."
        echo ""
        echo "Run this script again after fixing issues."
    fi
    
    create_readiness_report
    
    # Exit with appropriate code
    [ $failed_checks -eq 0 ] && exit 0 || exit 1
}

# Execute main function
main "$@"