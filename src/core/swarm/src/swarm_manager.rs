use crate::{
    Agent, AgentConfig, AgentId, AgentInfo, AgentMessage, AgentRole, AgentStatus, 
    ClusterInfo, HeartbeatData, PrismError, PrismResult
};\nuse async_trait::async_trait;\nuse chrono::{DateTime, Utc};\nuse dashmap::DashMap;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\nuse tokio::sync::{broadcast, mpsc, RwLock};\nuse tokio::time::interval;\nuse tracing::{debug, error, info, instrument, warn};\n\n/// Central coordinator for the PRISM agent swarm\n/// \n/// The SwarmManager maintains the registry of all agents in the local cluster,\n/// handles inter-agent communication, and coordinates distributed operations\n/// according to the PRISM PRD specifications.\npub struct SwarmManager {\n    /// Registry of all local agents (concurrent access)\n    local_agents: Arc<DashMap<AgentId, Arc<RwLock<dyn Agent>>>>,\n    \n    /// Information about peer agents from other nodes\n    peer_agents: Arc<DashMap<AgentId, AgentInfo>>,\n    \n    /// Configuration for swarm behavior\n    config: SwarmConfig,\n    \n    /// Current cluster leader (if known)\n    cluster_leader: Arc<RwLock<Option<AgentId>>>,\n    \n    /// Broadcast channel for inter-agent messages\n    message_tx: broadcast::Sender<AgentMessage>,\n    message_rx: broadcast::Receiver<AgentMessage>,\n    \n    /// Channel for internal swarm events\n    event_tx: mpsc::Sender<SwarmEvent>,\n    event_rx: Arc<RwLock<Option<mpsc::Receiver<SwarmEvent>>>>,\n    \n    /// Current consensus view number (for PBFT)\n    consensus_view: Arc<RwLock<u64>>,\n    \n    /// Swarm operational status\n    status: Arc<RwLock<SwarmStatus>>,\n    \n    /// Site identifier for multi-site deployments\n    site_id: Option<String>,\n}\n\n/// Configuration for swarm behavior\n#[derive(Debug, Clone)]\npub struct SwarmConfig {\n    /// Interval between heartbeat broadcasts\n    pub heartbeat_interval: Duration,\n    \n    /// Timeout before considering an agent failed\n    pub heartbeat_timeout: Duration,\n    \n    /// Maximum number of agents per swarm\n    pub max_agents: usize,\n    \n    /// Minimum number of master agents required\n    pub min_master_agents: usize,\n    \n    /// Maximum retry attempts for failed operations\n    pub max_retry_attempts: u32,\n    \n    /// Consensus timeout for distributed operations\n    pub consensus_timeout: Duration,\n}\n\nimpl Default for SwarmConfig {\n    fn default() -> Self {\n        Self {\n            heartbeat_interval: Duration::from_millis(50), // From PRD: 50ms heartbeat\n            heartbeat_timeout: Duration::from_millis(150), // 3 missed heartbeats\n            max_agents: 1000, // Reasonable upper bound\n            min_master_agents: 3, // From PRD: 3-5 master agents per site\n            max_retry_attempts: 3,\n            consensus_timeout: Duration::from_secs(5), // PBFT consensus timeout\n        }\n    }\n}\n\n/// Current status of the swarm\n#[derive(Debug, Clone)]\npub enum SwarmStatus {\n    Initializing,\n    Operational,\n    Degraded { reason: String },\n    Failed { reason: String },\n}\n\n/// Internal events within the swarm\n#[derive(Debug, Clone)]\npub enum SwarmEvent {\n    AgentJoined { agent_id: AgentId, role: AgentRole },\n    AgentLeft { agent_id: AgentId, reason: String },\n    AgentFailed { agent_id: AgentId, reason: String },\n    LeaderElected { leader_id: AgentId },\n    ConsensusViewChanged { new_view: u64 },\n    NetworkPartition { isolated_agents: Vec<AgentId> },\n}\n\nimpl SwarmManager {\n    /// Create a new SwarmManager with default configuration\n    pub fn new(site_id: Option<String>) -> Self {\n        Self::with_config(SwarmConfig::default(), site_id)\n    }\n    \n    /// Create a new SwarmManager with custom configuration\n    pub fn with_config(config: SwarmConfig, site_id: Option<String>) -> Self {\n        let (message_tx, message_rx) = broadcast::channel(1024);\n        let (event_tx, event_rx) = mpsc::channel(256);\n        \n        Self {\n            local_agents: Arc::new(DashMap::new()),\n            peer_agents: Arc::new(DashMap::new()),\n            config,\n            cluster_leader: Arc::new(RwLock::new(None)),\n            message_tx,\n            message_rx,\n            event_tx,\n            event_rx: Arc::new(RwLock::new(Some(event_rx))),\n            consensus_view: Arc::new(RwLock::new(0)),\n            status: Arc::new(RwLock::new(SwarmStatus::Initializing)),\n            site_id,\n        }\n    }\n    \n    /// Start the swarm manager and all its background tasks\n    #[instrument(skip(self))]\n    pub async fn start(&mut self) -> PrismResult<()> {\n        info!(\"Starting PRISM swarm manager\");\n        \n        // Update status to operational\n        {\n            let mut status = self.status.write().await;\n            *status = SwarmStatus::Operational;\n        }\n        \n        // Start heartbeat broadcast task\n        self.start_heartbeat_task().await;\n        \n        // Start failure detection task\n        self.start_failure_detection_task().await;\n        \n        // Start event processing task\n        self.start_event_processing_task().await;\n        \n        info!(\"PRISM swarm manager started successfully\");\n        Ok(())\n    }\n    \n    /// Register a new agent with the swarm\n    #[instrument(skip(self, agent))]\n    pub async fn register_agent(&self, agent: Arc<RwLock<dyn Agent>>) -> PrismResult<()> {\n        let agent_guard = agent.read().await;\n        let agent_id = agent_guard.agent_id();\n        let role = agent_guard.role();\n        \n        info!(\n            agent_id = %agent_id,\n            role = %role,\n            \"Registering new agent with swarm\"\n        );\n        \n        // Check if we're at capacity\n        if self.local_agents.len() >= self.config.max_agents {\n            return Err(PrismError::Validation {\n                message: format!(\n                    \"Swarm at capacity: {} agents (max: {})\", \n                    self.local_agents.len(),\n                    self.config.max_agents\n                )\n            });\n        }\n        \n        // Check for duplicate agent ID\n        if self.local_agents.contains_key(&agent_id) {\n            return Err(PrismError::Validation {\n                message: format!(\"Agent {} already registered\", agent_id)\n            });\n        }\n        \n        drop(agent_guard);\n        \n        // Register the agent\n        self.local_agents.insert(agent_id, agent);\n        \n        // Notify the swarm of the new agent\n        self.event_tx.send(SwarmEvent::AgentJoined { agent_id, role }).await\n            .map_err(|e| PrismError::Agent {\n                source: Box::new(e)\n            })?;\n        \n        debug!(agent_id = %agent_id, \"Agent registration complete\");\n        Ok(())\n    }\n    \n    /// Unregister an agent from the swarm\n    #[instrument(skip(self))]\n    pub async fn unregister_agent(&self, agent_id: AgentId, reason: String) -> PrismResult<()> {\n        info!(agent_id = %agent_id, reason = %reason, \"Unregistering agent from swarm\");\n        \n        // Remove from local registry\n        let removed = self.local_agents.remove(&agent_id);\n        \n        if removed.is_some() {\n            // Notify the swarm\n            self.event_tx.send(SwarmEvent::AgentLeft { agent_id, reason }).await\n                .map_err(|e| PrismError::Agent {\n                    source: Box::new(e)\n                })?;\n            \n            debug!(agent_id = %agent_id, \"Agent unregistration complete\");\n        } else {\n            warn!(agent_id = %agent_id, \"Attempted to unregister unknown agent\");\n        }\n        \n        Ok(())\n    }\n    \n    /// Get information about all agents in the swarm\n    pub async fn get_cluster_info(&self) -> ClusterInfo {\n        let total_agents = self.local_agents.len() + self.peer_agents.len();\n        let healthy_agents = self.count_healthy_agents().await;\n        let sites = self.get_unique_sites().await;\n        let leader_agent = *self.cluster_leader.read().await;\n        let consensus_view = *self.consensus_view.read().await;\n        \n        ClusterInfo {\n            cluster_id: self.get_cluster_id().await,\n            total_agents,\n            healthy_agents,\n            sites,\n            leader_agent,\n            consensus_view,\n        }\n    }\n    \n    /// Broadcast a message to all agents in the swarm\n    pub async fn broadcast_message(&self, message: AgentMessage) -> PrismResult<()> {\n        self.message_tx.send(message)\n            .map_err(|e| PrismError::Agent {\n                source: Box::new(e)\n            })?;\n        Ok(())\n    }\n    \n    /// Get the current swarm status\n    pub async fn status(&self) -> SwarmStatus {\n        self.status.read().await.clone()\n    }\n    \n    /// Start the heartbeat broadcasting task\n    async fn start_heartbeat_task(&self) {\n        let local_agents = Arc::clone(&self.local_agents);\n        let message_tx = self.message_tx.clone();\n        let interval_duration = self.config.heartbeat_interval;\n        \n        tokio::spawn(async move {\n            let mut interval = interval(interval_duration);\n            \n            loop {\n                interval.tick().await;\n                \n                // Collect heartbeats from all local agents\n                for agent_entry in local_agents.iter() {\n                    let agent = agent_entry.value();\n                    if let Ok(agent_guard) = agent.try_read() {\n                        let heartbeat = agent_guard.heartbeat();\n                        \n                        // Broadcast heartbeat to the swarm\n                        if let Err(e) = message_tx.send(AgentMessage::Heartbeat(heartbeat)) {\n                            error!(error = %e, \"Failed to broadcast heartbeat\");\n                        }\n                    }\n                }\n            }\n        });\n    }\n    \n    /// Start the failure detection task\n    async fn start_failure_detection_task(&self) {\n        let peer_agents = Arc::clone(&self.peer_agents);\n        let event_tx = self.event_tx.clone();\n        let timeout = self.config.heartbeat_timeout;\n        \n        tokio::spawn(async move {\n            let mut interval = interval(Duration::from_millis(100)); // Check every 100ms\n            \n            loop {\n                interval.tick().await;\n                let now = Utc::now();\n                \n                // Check for timed-out agents\n                let mut failed_agents = Vec::new();\n                \n                for agent_entry in peer_agents.iter() {\n                    let agent_info = agent_entry.value();\n                    let time_since_heartbeat = now - agent_info.last_heartbeat;\n                    \n                    if time_since_heartbeat.to_std().unwrap_or(Duration::MAX) > timeout {\n                        failed_agents.push((*agent_entry.key(), \n                            format!(\"Heartbeat timeout: {}ms\", time_since_heartbeat.num_milliseconds())));\n                    }\n                }\n                \n                // Process failed agents\n                for (agent_id, reason) in failed_agents {\n                    peer_agents.remove(&agent_id);\n                    \n                    if let Err(e) = event_tx.send(SwarmEvent::AgentFailed { agent_id, reason }).await {\n                        error!(error = %e, \"Failed to send agent failure event\");\n                    }\n                }\n            }\n        });\n    }\n    \n    /// Start the event processing task\n    async fn start_event_processing_task(&self) {\n        let event_rx = {\n            let mut rx_guard = self.event_rx.write().await;\n            rx_guard.take().expect(\"Event receiver should be available\")\n        };\n        \n        let status = Arc::clone(&self.status);\n        \n        tokio::spawn(async move {\n            let mut event_rx = event_rx;\n            \n            while let Some(event) = event_rx.recv().await {\n                match event {\n                    SwarmEvent::AgentJoined { agent_id, role } => {\n                        info!(agent_id = %agent_id, role = %role, \"Agent joined swarm\");\n                    }\n                    \n                    SwarmEvent::AgentLeft { agent_id, reason } => {\n                        info!(agent_id = %agent_id, reason = %reason, \"Agent left swarm\");\n                    }\n                    \n                    SwarmEvent::AgentFailed { agent_id, reason } => {\n                        warn!(agent_id = %agent_id, reason = %reason, \"Agent failed\");\n                    }\n                    \n                    SwarmEvent::LeaderElected { leader_id } => {\n                        info!(leader_id = %leader_id, \"New cluster leader elected\");\n                    }\n                    \n                    SwarmEvent::ConsensusViewChanged { new_view } => {\n                        info!(view = new_view, \"Consensus view changed\");\n                    }\n                    \n                    SwarmEvent::NetworkPartition { isolated_agents } => {\n                        warn!(\n                            isolated_count = isolated_agents.len(),\n                            \"Network partition detected\"\n                        );\n                        \n                        // Update swarm status to degraded\n                        let mut status_guard = status.write().await;\n                        *status_guard = SwarmStatus::Degraded {\n                            reason: format!(\"Network partition: {} agents isolated\", isolated_agents.len())\n                        };\n                    }\n                }\n            }\n        });\n    }\n    \n    /// Count healthy agents across the entire swarm\n    async fn count_healthy_agents(&self) -> usize {\n        let mut count = 0;\n        \n        // Count local agents\n        for agent_entry in self.local_agents.iter() {\n            let agent = agent_entry.value();\n            if let Ok(agent_guard) = agent.try_read() {\n                if matches!(agent_guard.status(), AgentStatus::Healthy) {\n                    count += 1;\n                }\n            }\n        }\n        \n        // Count peer agents\n        for agent_entry in self.peer_agents.iter() {\n            if matches!(agent_entry.value().status, AgentStatus::Healthy) {\n                count += 1;\n            }\n        }\n        \n        count\n    }\n    \n    /// Get list of unique sites in the cluster\n    async fn get_unique_sites(&self) -> Vec<String> {\n        let mut sites = std::collections::HashSet::new();\n        \n        // Add local site\n        if let Some(site) = &self.site_id {\n            sites.insert(site.clone());\n        }\n        \n        // Add peer sites\n        for agent_entry in self.peer_agents.iter() {\n            if let Some(site) = &agent_entry.value().site_id {\n                sites.insert(site.clone());\n            }\n        }\n        \n        sites.into_iter().collect()\n    }\n    \n    /// Generate a cluster ID based on the swarm state\n    async fn get_cluster_id(&self) -> String {\n        format!(\"prism-cluster-{}\", \n            self.site_id.as_deref().unwrap_or(\"default\")\n        )\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::{AgentConfig, AgentRole};\n    use tokio_test;\n    \n    struct MockAgent {\n        id: AgentId,\n        role: AgentRole,\n        status: AgentStatus,\n    }\n    \n    #[async_trait]\n    impl Agent for MockAgent {\n        fn agent_id(&self) -> AgentId { self.id }\n        fn role(&self) -> AgentRole { self.role }\n        fn status(&self) -> AgentStatus { self.status.clone() }\n        \n        fn heartbeat(&self) -> HeartbeatData {\n            HeartbeatData {\n                agent_id: self.id,\n                timestamp: Utc::now(),\n                sequence_number: 0,\n                status: self.status.clone(),\n                load_metrics: Default::default(),\n                site_id: None,\n            }\n        }\n        \n        async fn start(&mut self) -> PrismResult<()> { Ok(()) }\n        async fn stop(&mut self) -> PrismResult<()> { Ok(()) }\n        async fn handle_message(&mut self, _: AgentMessage) -> PrismResult<()> { Ok(()) }\n        async fn health_check(&mut self) -> PrismResult<AgentStatus> { Ok(self.status.clone()) }\n    }\n    \n    #[tokio::test]\n    async fn test_swarm_manager_creation() {\n        let swarm = SwarmManager::new(Some(\"site-a\".to_string()));\n        assert!(matches!(swarm.status().await, SwarmStatus::Initializing));\n    }\n    \n    #[tokio::test]\n    async fn test_agent_registration() {\n        let swarm = SwarmManager::new(None);\n        \n        let agent = Arc::new(RwLock::new(MockAgent {\n            id: AgentId::new(),\n            role: AgentRole::Master,\n            status: AgentStatus::Healthy,\n        }));\n        \n        let result = swarm.register_agent(agent).await;\n        assert!(result.is_ok());\n        \n        let cluster_info = swarm.get_cluster_info().await;\n        assert_eq!(cluster_info.total_agents, 1);\n    }\n    \n    #[tokio::test]\n    async fn test_duplicate_agent_registration() {\n        let swarm = SwarmManager::new(None);\n        let agent_id = AgentId::new();\n        \n        let agent1 = Arc::new(RwLock::new(MockAgent {\n            id: agent_id,\n            role: AgentRole::Master,\n            status: AgentStatus::Healthy,\n        }));\n        \n        let agent2 = Arc::new(RwLock::new(MockAgent {\n            id: agent_id, // Same ID\n            role: AgentRole::Worker,\n            status: AgentStatus::Healthy,\n        }));\n        \n        // First registration should succeed\n        assert!(swarm.register_agent(agent1).await.is_ok());\n        \n        // Second registration should fail\n        assert!(swarm.register_agent(agent2).await.is_err());\n    }\n}"